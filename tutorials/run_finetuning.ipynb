{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning MIST Encoder Models\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/BattModels/mist-demo/blob/main/tutorials/run_finetuning.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "The core advantage of a foundation model is that it can be adapted to a wide range of downstream tasks given a small number of labelled examples.\n",
    "We have demonstrated the MIST models' efficacy as scientific foundation models by fine-tuning variants of MIST to predict over 400 properties --- including quantum mechanical, thermodynamic, biochemical, and psychophysical properties --- from a molecule’s SMILES representation.\n",
    "The encoders are fine-tuned on single molecule property prediction (classification and regression) tasks by attaching a small two-layer MLP. \n",
    "\n",
    "This tutorial demonstrates how to fine-tune MIST encoder models for downstream molecular property prediction tasks.\n",
    "As an examples, we will finetune a MIST encoder to predict LUMO (Lowest Unoccupied Molecular Orbital) energies from the QM9 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# ! pip install git+https://github.com/BattModels/mist-demo.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from smirk import SmirkTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "from mist_demo.finetuning.optimize_hyperparams import tune_hyperparameters\n",
    "from mist_demo.finetuning.regression_model import RegressionModel\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "MIST models were pretrained on *kekulized SMILES*, which is a canonical form that explicitly represents alternating single and double bonds in aromatic rings. We need to kekulize our input SMILES strings before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kekulize_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES: {smiles}\")\n",
    "    Chem.Kekulize(mol)\n",
    "    return Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_smiles = \"c1ccccc1\"  # Benzene with aromatic notation\n",
    "kekulized = kekulize_smiles(test_smiles)\n",
    "print(f\"Original: {test_smiles}\")\n",
    "print(f\"Kekulized: {kekulized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function to tokenize SMILES strings with proper kekulization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    kekulized = [kekulize_smiles(s) for s in examples[\"smiles\"]]\n",
    "    return tokenizer(\n",
    "        kekulized,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\"},\n",
    ")[\"train\"]\n",
    "\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "# MIST models were trained using the Smirk tokenizer\n",
    "\n",
    "tokenizer = SmirkTokenizerFast()\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenizer,\n",
    "    input_columns=[\"smiles\"],\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "\n",
    "# Rename target column to 'labels' (expected by Trainer)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"lumo\", \"labels\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "tokenized_dataset[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pretrained MIST encoder \n",
    "\n",
    "We'll load a pretrained MIST encoder from HuggingFace Hub. You can choose from two different model sizes:\n",
    "- `mist-models/mist-1.8B-dh61satt` (1.8B parameters)\n",
    "- `mist-models/mist-28M-ti624ev1` (1.8B parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mist-models/mist-28M-ti624ev1\"\n",
    "\n",
    "print(f\"Loading model: {model_path}\")\n",
    "encoder = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# This will be much faster and use less memory than the full model!\n",
    "print(f\"Encoder hidden size: {encoder.config.hidden_size}\")\n",
    "print(f\"Number of encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA for efficient fine-tuning\n",
    "LoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable adapter layers while keeping the base model frozen. This dramatically reduces training time and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                              # Rank of LoRA matrices\n",
    "    lora_alpha=16,                    # Scaling factor\n",
    "    target_modules=[\"query\", \"value\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,                 # Dropout for LoRA layers\n",
    "    bias=\"none\",                      # Don't train bias terms\n",
    "    task_type=None,                   # Custom task type\n",
    ")\n",
    "\n",
    "# Apply LoRA to encoder\n",
    "encoder = get_peft_model(encoder, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "encoder.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining an architecture for fine-tuning\n",
    "We will create a `RegressionModel` that combines:\n",
    "- A pretrained MIST encoder\n",
    "- A MLP (multi-layer perceptron) regression head with tunable architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Task Network\n",
    "\n",
    "Before training, we can use Bayesian optimization to find optimal hyperparameters for the task network. We'll tune:\n",
    "- **Dropout rate**: Regularization strength\n",
    "- **Number of hidden layers**: Model complexity\n",
    "- **Learning rate**: Optimization step size\n",
    "- **Batch size**: Training batch size\n",
    "\n",
    "We will use [Optuna's TPE (Tree-structured Parzen Estimator)](https://hub.optuna.org/samplers/tpe_tutorial/) sampler for efficient Bayesian optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning\n",
    "# This will take some time - adjust n_trials based on your compute budget\n",
    "\n",
    "best_params, study = tune_hyperparameters(\n",
    "    encoder=encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    val_dataset=tokenized_dataset[\"test\"],\n",
    "    device=device,\n",
    "    n_trials=5  # Increase for better results\n",
    ")\n",
    "\n",
    "# If you don't want to run hyperparameter tuning\n",
    "# we'll these use default parameters\n",
    "# best_params = {\n",
    "#     'task_hidden_size': 512,\n",
    "#     'dropout': 0.1,\n",
    "#     'num_hidden_layers': 1,\n",
    "#     'learning_rate': 1.6e-4,\n",
    "#     'batch_size': 32\n",
    "# }\n",
    "\n",
    "print(\"Using hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Optimization Results\n",
    "\n",
    "Let's analyze the impact of hyperparameters on the model's validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig1.update_layout(template='simple_white', height = 400, width = 700)\n",
    "fig1.update_yaxes(type=\"log\")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom visualization of all trials\n",
    "trials_df = study.trials_dataframe()\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    trials_df,\n",
    "    dimensions=['params_dropout', 'params_num_hidden_layers', \n",
    "                'params_learning_rate', 'params_batch_size', 'value'],\n",
    "    color='value',\n",
    "    color_continuous_scale='Viridis',\n",
    "    labels={\n",
    "        'params_task_hidden_size': 'Hidden Size',\n",
    "        'params_dropout': 'Dropout',\n",
    "        'params_num_hidden_layers': '# Layers',\n",
    "        'params_learning_rate': 'Learning Rate',\n",
    "        'params_batch_size': 'Batch Size',\n",
    "        'value': 'Val Loss'\n",
    "    }\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='simple_white',\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with task head using tuned hyperparameters\n",
    "model = RegressionModel(\n",
    "    encoder=encoder,\n",
    "    hidden_size=encoder.config.hidden_size,\n",
    "    dropout=best_params['dropout'],\n",
    "    num_hidden_layers=best_params['num_hidden_layers']\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Freeze encoder parameters\n",
    "# Only task head will be trained\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model = model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  - LoRA parameters: {sum(p.numel() for p in encoder.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  - Task head parameters: {sum(p.numel() for p in model.task_head.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training arguments using tuned hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=best_params['batch_size'],\n",
    "    per_device_eval_batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup ratio: {training_args.warmup_ratio}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total training steps: {len(tokenized_dataset['train']) // best_params['batch_size'] * training_args.num_train_epochs}\")\n",
    "print(f\"Evaluation every: {len(tokenized_dataset['train']) // best_params['batch_size']} steps\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the finetuned model\n",
    "save_path = \"./finetuned_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "Let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Prepare test subset and dataloader\n",
    "test_subset = tokenized_dataset[\"test\"].select(range(min(1000, len(tokenized_dataset[\"test\"]))))\n",
    "test_dataloader = DataLoader(test_subset, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "# Collect predictions and labels\n",
    "test_predictions, test_labels = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        preds = model(**inputs)[\"y_pred\"].squeeze(-1).cpu().numpy()\n",
    "        \n",
    "        test_predictions.extend(preds)\n",
    "        test_labels.extend(batch['labels'].numpy())\n",
    "\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = np.mean(np.abs(test_predictions - test_labels))\n",
    "rmse = np.sqrt(np.mean((test_predictions - test_labels)**2))\n",
    "r2 = 1 - np.sum((test_labels - test_predictions)**2) / np.sum((test_labels - test_labels.mean())**2)\n",
    "\n",
    "print(f\"Test Set Performance (n={len(test_predictions)}):\")\n",
    "print(f\"  MAE:  {mae:.4f} Hartree\")\n",
    "print(f\"  RMSE: {rmse:.4f} Hartree\")\n",
    "print(f\"  R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Scatter plot of predictions vs actual\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=test_labels,\n",
    "    y=test_predictions,\n",
    "    mode='markers',\n",
    "    name='Predictions',\n",
    "    marker=dict(\n",
    "        size=6,\n",
    "        color=np.abs(test_predictions - test_labels),\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Absolute Error\"),\n",
    "        opacity=0.6,\n",
    "        line=dict(width=0.5, color='white')\n",
    "    ),\n",
    "))\n",
    "\n",
    "# Add perfect prediction line (y = x)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=test_labels,\n",
    "    y=test_labels,\n",
    "    mode='lines',\n",
    "    name='Perfect Prediction',\n",
    "    line=dict(color='red', width=2, dash='dash'),\n",
    "    hoverinfo='skip'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title='Actual LUMO (Hartree)',\n",
    "    yaxis_title='Predicted LUMO (Hartree)',\n",
    "    template='simple_white',\n",
    "    height=600,\n",
    "    width=700,\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Make axes equal\n",
    "fig.update_xaxes(scaleanchor=\"y\", scaleratio=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
