train:
  tags: [pretraining]

  # Model Size
  model: RoBERTa
  model.intermediate_size: 768
  model.hidden_size: 768
  model.num_attention_heads: 12
  model.num_hidden_layers: 12

  # Dataset
  data: RobertaDataSet
  data.batch_size: 128
  data.val_batch_size: 2048
  data.tokenizer: smirk

  # Optimizer Settings
  model.lr_schedule: electrolyte_fm.utils.lr_schedule.RelativeCosineWarmup
  model.lr_schedule.num_training_steps: 50_000
  model.lr_schedule.num_warmup_steps: beta2
  model.lr_schedule.rel_decay: 0.1

  # Trainer Configuration
  trainer.max_steps: 10_000
  trainer.val_check_interval: 100
  trainer.precision: bf16-true
  trainer.enable_progress_bar: False

env:
  TOKENIZERS_PARALLELISM: "true"

deepspeed:
  optimizer:
    type: Adam
    params:
      lr: 0.00016
      weight_decay: 0.01

  zero_optimization:
    stage: 3
    offload_optimizer:
      device: cpu
    offload_param:
      device: cpu
    contiguous_gradients: true
    overlap_comm: true
