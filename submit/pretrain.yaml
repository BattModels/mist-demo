train::
  data: RobertaDataSet
  data.path: /home/awadell/realspace_v3_dev/
  data.num_workers: 1  # Only have one shard per GPU in realspace_v2_dev\
  data.batch_size: 512
  data.tokenizer: ibm/MoLFormer-XL-both-10pct
  model: RoBERTa
  model.num_attention_heads: 6
  model.num_hidden_layers: 3
  model.hidden_size: 768
  model.intermediate_size: 1536
  model.optimizer: torch.optim.Adam
  model.optimizer.lr: 1e-4
  model.optimizer.weight_decay: 0.01
  model.lr_schedule: electrolyte_fm.utils.lr_schedule.RelativeCosineWarmup
  model.lr_schedule.num_training_steps: 200000
  model.lr_schedule.num_warmup_steps: beta2
  model.lr_schedule.rel_decay: 0.1
