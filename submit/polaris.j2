#!/bin/sh
{%- from "./cliopts.j2" import cliopts %}
#PBS -N {{ jobname or "MIST" }}
#PBS -l select={{ nodes or 1 }}:system=polaris
#PBS -l place=scatter
#PBS -l filesystems=eagle:home
#PBS -l walltime=0:20:00
#PBS -q {{ queue or "debug" }}
#PBS -A {{ account or "FoundEnergy" }}
set -x
cd ${PBS_O_WORKDIR}

# Internet access on nodes
export HTTPS_PROXY=http://proxy.alcf.anl.gov:3130
export HTTP_PROXY=http://proxy.alcf.anl.gov:3128
export http_proxy=http://proxy.alcf.anl.gov:3128
export https_proxy=http://proxy.alcf.anl.gov:3128
git config --global http.proxy http://proxy.alcf.anl.gov:3128
echo "Set HTTP_PROXY and to $HTTP_PROXY"

# Set ADDR and PORT for communication
master_node=$(cat $PBS_NODEFILE | head -1)
export MASTER_ADDR=$(host $master_node | head -1 | awk '{print $4}')
export MASTER_PORT=2345
# Enable GPU-MPI (if supported by application)
export MPICH_GPU_SUPPORT_ENABLED=1
# MPI and OpenMP settings
NNODES=$(wc -l <$PBS_NODEFILE)
NRANKS_PER_NODE=1
NDEPTH=64
NTOTRANKS=$((NNODES * NRANKS_PER_NODE))
echo "NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE}"
echo <$PBS_NODEFILE

export NRANKS=$(wc -l <"${PBS_NODEFILE}") 
# export NODE_RANK=$((${PMI_RANK} % ${NRANKS}))
# export GLOBAL_RANK=$((${PMI_RANK} % ${NRANKS}))
# export RANK=$((${PMI_RANK} % ${NRANKS})) # Experimental wandb fix

# Initialize environment
module load conda
conda activate base
source ${PBS_O_WORKDIR}/activate

export TOKENIZERS_PARALLELISM=false

# Logging
echo "$(df -h /local/scratch)"

# NCCL settings
export NCCL_DEBUG=WARN
export NCCL_NET_GDR_LEVEL=PHB
export NCCL_COLLNET_ENABLE=1

# For applications that internally handle binding MPI/OpenMP processes to GPUs
{%- if nsys is defined %}
nsys profile \
      {{- cliopts( nsys ) | indent }} \
{%- endif %}
mpiexec \
    {{- cliopts( mpiexe ) | indent }} \
    python3 train.py fit \
    {{- cliopts( train ) | indent }}
