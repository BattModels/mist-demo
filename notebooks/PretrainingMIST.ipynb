{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from mist.models.roberta_base import RoBERTa\n",
    "from mist.data_modules.roberta_dataset import RobertaDataSet\n",
    "from mist.utils.lr_schedule import RelativeCosineWarmup\n",
    "\n",
    "# enable RUST based parallelism for tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training Dataset\n",
    "\n",
    "We use a subset of randomly sampled molecules from [Enamine’s REAL Space Chemical Library](https://enamine.net/compound-collections/real-compounds/real-space-navigator), which is currently the largest library of commercially available compounds with 48B virtual products based on ~0.1M reagents and building blocks and 166 defined chemical rules to combine them. \n",
    "\n",
    "This pre-training dataset covers a significant fraction of the space of possible molecules. The plot below visualizes the chemical space covered by the pre-training dataset using the [TMAP](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-0416-x) (Tree Manifold Approximation and Projection) algorithm and compare it to the chemical space convered by datasets in MoleculeNet. MoleculeNet is a popular cheminformatics benchmark and is representative of datasets typically used to train machine learning models for chemistry.\n",
    "\n",
    "<img src=\"figures/MIST_TMAP.png\" alt=\"tmap\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">\n",
    "\n",
    "\n",
    "The molecules are stored as SMILES (Simplified Molecular-Input Line-Entry System) strings. SMILES are a cheminformatic line notation for describing chemical structures using short ASCII strings. SMILES strings are like a connection table in that they identify the nodes and edges of a molecular graph. In SMILES, hydrogen are typically implicitly implied and atoms are represented by their atomic symbol enclosed in brackets unless they are elements of the “organic subset” (`B`, `C`, `N`, `O`, `P`, `S`, `F`, `Cl`,`Br`, and `I`), which do not require brackets unless they are charged. So gold would be `[Au]` but chlorine would be just `Cl`. If hydrogens are explicitly implied brackets are used. A formal charge is represented by one of the symbols `+` or `-`. Single, double, triple, and aromatic bonds are represented by the symbols, `-`, `=`, `#`, and `:`, respectively. Single and aromatic bonds may be, and usually are, omitted. Below is an example of a SMILES string and the corresponding 2D molecular graph.\n",
    "\n",
    "<img src=\"figures/smiles.png\" alt=\"smiles\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_data_path = \"/eagle/FoundEnergy/realspace_v2\"\n",
    "tokenizer = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "mlm_probability = 0.15 \n",
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "\n",
    "datamodule = RobertaDataSet(\n",
    "    path=pretraining_data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    val_batch_size=val_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = datamodule.tokenizer.vocab_size\n",
    "max_position_embeddings = 512\n",
    "num_attention_heads = 12\n",
    "num_attention_heads = 6\n",
    "num_hidden_layers = 6\n",
    "hidden_size = 768\n",
    "intermediate_size = 768\n",
    "relative_cosine_scheduler = lambda optimizer: RelativeCosineWarmup(optimizer, num_warmup_steps=\"beta2\", num_training_steps=50_000)\n",
    "\n",
    "model = RoBERTa(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_position_embeddings,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    optimizer = torch.optim.AdamW,\n",
    "    lr_schedule = relative_cosine_scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some callbacks are defined for convinience\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\") # monitors and logs learning rate for schedulers during training\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_last=\"link\",\n",
    "        filename=\"epoch={epoch}-step={step}-val_loss={val/loss_epoch:.2f}\",\n",
    "        monitor=\"val/loss_epoch\",\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        auto_insert_metric_name=False\n",
    "    ) # saves the best model during training based on validation loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "The pre-training strategy we use is analogous the MLM (Masked Language Modeling) used in NLP (Natural Language Processing). \n",
    "Part of the SMILES string is replace with a 'mask'. The objective is a cross-entropy loss on predicting the masked tokens.\n",
    "\n",
    "<img src=\"figures/MIST_pretraining.png\" alt=\"tmap\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    precision = \"16-mixed\", # Combines FP32 and lower-bit floating points to reduce memory footprint and increase performance.\n",
    "    strategy = \"ddp_notebook\", # Distributed Data Parallel training.\n",
    "    use_distributed_sampler = False,  # Handled by DataModule (needed due to IterableDataset).\n",
    "    limit_train_batches=5000, # Limit train batches for shorter training time \n",
    "    limit_val_batches=10, \n",
    "    max_epochs=1, \n",
    "    devices=torch.cuda.device_count(),\n",
    "    callbacks=[lr_monitor, checkpoint_callback])\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup(stage=\"test\")\n",
    "for step, sample in enumerate(datamodule.val_dataloader()):\n",
    "    print(\"Masked Molecule\", datamodule.tokenizer.decode(sample['input_ids'].flatten()))\n",
    "    mask = sample['labels'].flatten()!=-100\n",
    "    labels = sample['labels'].flatten()[mask]\n",
    "    print(\"Labels\", datamodule.tokenizer.convert_ids_to_tokens(labels))\n",
    "    pred = trainer.model.model(\n",
    "        input_ids=sample['input_ids'],\n",
    "        attention_mask = sample['attention_mask'],\n",
    "    )\n",
    "    pred = pred.logits[0].argmax(axis=1)[mask]\n",
    "    pred = datamodule.tokenizer.convert_ids_to_tokens(pred)\n",
    "    print(\"Labels\", pred)\n",
    "    print(\"_\"*150)\n",
    "    if step > 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mist_demo",
   "language": "python",
   "name": "mist_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
