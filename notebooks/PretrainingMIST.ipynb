{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from electrolyte_fm.models import RoBERTa\n",
    "from electrolyte_fm.data_modules import RobertaDataSet\n",
    "from electrolyte_fm.utils.lr_schedule import RelativeCosineWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = \"/nfs/turbo/coe-venkvis/mist/realspace_v3_dev/\"\n",
    "tokenizer = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "mlm_probability = 0.15 \n",
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "\n",
    "\n",
    "datamodule = RobertaDataSet(\n",
    "    path=path,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    val_batch_size=val_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = datamodule.tokenizer.vocab_size\n",
    "max_position_embeddings = 512\n",
    "num_attention_heads = 12\n",
    "num_attention_heads = 6\n",
    "num_hidden_layers = 6\n",
    "hidden_size = 768\n",
    "intermediate_size = 768\n",
    "relative_cosine_scheduler = lambda optimizer: RelativeCosineWarmup(optimizer, num_warmup_steps=\"beta2\", num_training_steps=50_000)\n",
    "\n",
    "model = RoBERTa(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_position_embeddings,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    optimizer = torch.optim.AdamW,\n",
    "    lr_schedule = relative_cosine_scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some callbacks are defined for convinience\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\") # monitors and logs learning rate for schedulers during training\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_last=\"link\",\n",
    "        filename=\"epoch={epoch}-step={step}-val_loss={val/loss_epoch:.2f}\",\n",
    "        monitor=\"val/loss_epoch\",\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        auto_insert_metric_name=False\n",
    "    ) # saves the best model during training based on validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/abhutani/electrolyte_fm/.venv/lib/python3.11/s ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | RobertaForMaskedLM | 24.1 M\n",
      "---------------------------------------------\n",
      "24.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.1 M    Total params\n",
      "96.335    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                           | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████| 500/500 [01:15<00:00,  6.63it/s, v_num=12, train/loss_step=1.040]\n",
      "Validation: |                                                              | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|                                                         | 0/10 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|                                            | 0/10 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  10%|███▌                                | 1/10 [00:00<00:00, 15.53it/s]\n",
      "Validation DataLoader 0:  20%|███████▏                            | 2/10 [00:00<00:00, 14.28it/s]\n",
      "Validation DataLoader 0:  30%|██████████▊                         | 3/10 [00:00<00:00, 15.58it/s]\n",
      "Validation DataLoader 0:  40%|██████████████▍                     | 4/10 [00:00<00:00, 17.06it/s]\n",
      "Validation DataLoader 0:  50%|██████████████████                  | 5/10 [00:00<00:00, 17.70it/s]\n",
      "Validation DataLoader 0:  60%|█████████████████████▌              | 6/10 [00:00<00:00, 17.99it/s]\n",
      "Validation DataLoader 0:  70%|█████████████████████████▏          | 7/10 [00:00<00:00, 18.35it/s]\n",
      "Validation DataLoader 0:  80%|████████████████████████████▊       | 8/10 [00:00<00:00, 18.62it/s]\n",
      "Validation DataLoader 0:  90%|████████████████████████████████▍   | 9/10 [00:00<00:00, 17.77it/s]\n",
      "Validation DataLoader 0: 100%|███████████████████████████████████| 10/10 [00:00<00:00, 18.30it/s]\n",
      "Epoch 0: 100%|█| 500/500 [01:16<00:00,  6.53it/s, v_num=12, train/loss_step=1.040, val/loss_step="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 500: 'val/loss_epoch' reached inf (best inf), saving model to '/home/abhutani/electrolyte_fm/lightning_logs/version_12/checkpoints/epoch=0-step=500-val_loss=nan.ckpt' as top 5\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 500/500 [01:19<00:00,  6.26it/s, v_num=12, train/loss_step=1.040, val/loss_step=\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    precision = \"16-mixed\", # Combines FP32 and lower-bit floating points to reduce memory footprint and increase performance.\n",
    "    strategy = \"ddp_notebook\", # Distributed Data Parallel training.\n",
    "    use_distributed_sampler = False,  # Handled by DataModule (needed due to IterableDataset).\n",
    "    limit_train_batches=500, \n",
    "    limit_val_batches=10, \n",
    "    max_epochs=1, \n",
    "    devices=torch.cuda.device_count(),\n",
    "    callbacks=[lr_monitor, checkpoint_callback])\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Molecule <bos>CCS(=O<mask>(=[77Kr])NCC1=CN(CC<mask>CCOCCNC(=<mask>)CN2N=C<mask>C[AlH4-]C<mask>N3C(=O)C2=O)N=N1<eos>\n",
      "Labels ['=', ')', 'O', 'O', 'O', '3', 'O', 'C']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>COC(=O)C(<mask>C1=CN(CCN2CCC(N(C)C)C<mask><mask>N=N1)<mask>C(=<mask>)<mask>N1C=C(C<mask>N)=O)C[117Sn+4]<mask>O)<mask>C1=O<eos>\n",
      "Labels ['C', 'N', '2', ')', 'N', 'O', 'C', '(', '(', '=', 'N']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>NC<mask>=O)CCC(N<mask>(=O)(=O)C1=CC=C(Cl<mask>C<mask>C1<mask>C<mask><mask><mask>)N[C@@H]<mask>CO)CNC(=<mask>)CN1N=CC(=O<mask>NC1=O<eos>\n",
      "Labels ['(', 'C', 'S', ')', '=', ')', '(', '=', 'O', '(', 'O', ')']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>CN<mask>C(=O<mask>C=CN(CC(=O<mask>NCC(O)C<mask>C(=O)C(CC(<mask>)=O)NC(<mask>O)CC2=CC=CC(F)=C2)C1=O<eos>\n",
      "Labels ['1', ')', ')', 'N', 'N', '=', ')', 'C']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>CN<mask>C(C(<mask>C<mask>CN([137Ba](C)(=O)=O)C[Co-])C2=NN=NN2CC<mask>2CCO<mask>C2<mask>=NC2=C1C(=O)<mask>C(=<mask>)N2C<eos>\n",
      "Labels ['1', 'N', '2', 'S', '2', 'N', 'C', ')', 'N', 'O']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for step, sample in enumerate(datamodule.val_dataloader()):\n",
    "    print(\"Masked Molecule\", datamodule.tokenizer.decode(sample['input_ids'].flatten()))\n",
    "    mask = sample['labels'].flatten()!=-100\n",
    "    labels = sample['labels'].flatten()[mask]\n",
    "    print(\"Labels\", datamodule.tokenizer.convert_ids_to_tokens(labels))\n",
    "    pred = model(\n",
    "        sample\n",
    "    )\n",
    "    pred = pred.logits[0].argmax(axis=1)[mask]\n",
    "    pred = datamodule.tokenizer.convert_ids_to_tokens(pred)\n",
    "    print(\"Labels\", pred)\n",
    "    print(\"_\"*200)\n",
    "    if step > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mist_kernel",
   "language": "python",
   "name": "mist_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
