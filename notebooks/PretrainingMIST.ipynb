{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 16:22:17.710192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 16:22:19.153266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from mist.models.roberta_base import RoBERTa\n",
    "from mist.data_modules.roberta_dataset import RobertaDataSet\n",
    "from mist.utils.lr_schedule import RelativeCosineWarmup\n",
    "\n",
    "# enable RUST based parallelism for tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training Dataset\n",
    "\n",
    "We use a subset of randomly sampled molecules from [Enamine’s REAL Space Chemical Library](https://enamine.net/compound-collections/real-compounds/real-space-navigator), which is currently the largest library of commercially available compounds with 48B virtual products based on ~0.1M reagents and building blocks and 166 defined chemical rules to combine them. \n",
    "\n",
    "This pre-training dataset covers a significant fraction of the space of possible molecules. The plot below visualizes the chemical space covered by the pre-training dataset using the [TMAP](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-0416-x) (Tree Manifold Approximation and Projection) algorithm and compare it to the chemical space convered by datasets in MoleculeNet. MoleculeNet is a popular cheminformatics benchmark and is representative of datasets typically used to train machine learning models for chemistry.\n",
    "\n",
    "<img src=\"figures/MIST_TMAP.png\" alt=\"tmap\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">\n",
    "\n",
    "\n",
    "The molecules are stored as SMILES (Simplified Molecular-Input Line-Entry System) strings. SMILES are a cheminformatic line notation for describing chemical structures using short ASCII strings. SMILES strings are like a connection table in that they identify the nodes and edges of a molecular graph. In SMILES, hydrogen are typically implicitly implied and atoms are represented by their atomic symbol enclosed in brackets unless they are elements of the “organic subset” (`B`, `C`, `N`, `O`, `P`, `S`, `F`, `Cl`,`Br`, and `I`), which do not require brackets unless they are charged. So gold would be `[Au]` but chlorine would be just `Cl`. If hydrogens are explicitly implied brackets are used. A formal charge is represented by one of the symbols `+` or `-`. Single, double, triple, and aromatic bonds are represented by the symbols, `-`, `=`, `#`, and `:`, respectively. Single and aromatic bonds may be, and usually are, omitted. Below is an example of a SMILES string and the corresponding 2D molecular graph.\n",
    "\n",
    "<img src=\"figures/smiles.png\" alt=\"smiles\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_data_path = \"/eagle/FoundEnergy/realspace_v2\"\n",
    "tokenizer = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "mlm_probability = 0.15 \n",
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "\n",
    "datamodule = RobertaDataSet(\n",
    "    path=pretraining_data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    val_batch_size=val_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = datamodule.tokenizer.vocab_size\n",
    "max_position_embeddings = 512\n",
    "num_attention_heads = 12\n",
    "num_attention_heads = 6\n",
    "num_hidden_layers = 6\n",
    "hidden_size = 768\n",
    "intermediate_size = 768\n",
    "relative_cosine_scheduler = lambda optimizer: RelativeCosineWarmup(optimizer, num_warmup_steps=\"beta2\", num_training_steps=50_000)\n",
    "\n",
    "model = RoBERTa(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_position_embeddings,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    optimizer = torch.optim.AdamW,\n",
    "    lr_schedule = relative_cosine_scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some callbacks are defined for convinience\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\") # monitors and logs learning rate for schedulers during training\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_last=\"link\",\n",
    "        filename=\"epoch={epoch}-step={step}-val_loss={val/loss_epoch:.2f}\",\n",
    "        monitor=\"val/loss_epoch\",\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        auto_insert_metric_name=False\n",
    "    ) # saves the best model during training based on validation loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "The pre-training strategy we use is analogous the MLM (Masked Language Modeling) used in NLP (Natural Language Processing). \n",
    "Part of the SMILES string is replace with a 'mask'. The objective is a cross-entropy loss on predicting the masked tokens.\n",
    "\n",
    "<img src=\"figures/MIST_pretraining.png\" alt=\"tmap\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd606f75f1a483f88a82e60711ce24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0965b94bd97444d9629c4f097f9f5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a6262d11174a11a63a8e974d3b0c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ec64a9b20243779895c0a30d52b18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4767dbb168684068bcc23bc9239bb5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7375635c77104a76ab1bb904bc4686d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af208a2ac6e74fb3a11b6c9025c18d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bf1d91e84e400d9d2b33c2c7aa4e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adba159d4304c828a7fca077fff7159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce5198ed5f44b9c96b1555d5c3b4291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c96fa4c3564319995d29312782c6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e137c313db994d32bd432f668e645cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type               | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model | RobertaForMaskedLM | 24.1 M | train\n",
      "-----------------------------------------------------\n",
      "24.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.1 M    Total params\n",
      "96.335    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab7cdc78156424d8a3bdb45c59a2c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/mist/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/abhutani/mist/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320d4fcece6c4d76a5a340f7fcbf8811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a22022be1f24f4aaf01ec438234098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 5000: 'val/loss_epoch' reached 2.20291 (best 2.20291), saving model to '/home/abhutani/mist/notebooks/lightning_logs/version_5/checkpoints/epoch=0-step=5000-val_loss=2.20.ckpt' as top 5\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    precision = \"16-mixed\", # Combines FP32 and lower-bit floating points to reduce memory footprint and increase performance.\n",
    "    strategy = \"ddp_notebook\", # Distributed Data Parallel training.\n",
    "    use_distributed_sampler = False,  # Handled by DataModule (needed due to IterableDataset).\n",
    "    limit_train_batches=5000, # Limit train batches for shorter training time \n",
    "    limit_val_batches=10, \n",
    "    max_epochs=1, \n",
    "    devices=torch.cuda.device_count(),\n",
    "    callbacks=[lr_monitor, checkpoint_callback])\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d35e94d20743b9a2eb82f6edff2459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e670fe39c4b649839bd63198ed66c65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea8b6fa9c1e487da6cde8870e7ab0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Molecule <bos>C[13C-]S[Se@@]=O)(<mask>O<mask>NCC1=CN(CCO<mask>COCCNC(=O)CN2N=C3<mask>O<mask>CN3<mask>(=O<mask>C2=O)[n-]=<mask>1<eos>\n",
      "Labels ['C', '(', '=', ')', 'C', 'C', 'C', 'C', ')', 'N', 'N']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RoBERTa' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()[mask]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, datamodule\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(labels))\n\u001b[0;32m----> 7\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m(\n\u001b[1;32m      8\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[mask]\n\u001b[1;32m     12\u001b[0m pred \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(pred)\n",
      "File \u001b[0;32m~/mist/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RoBERTa' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "datamodule.setup(stage=\"test\")\n",
    "for step, sample in enumerate(datamodule.val_dataloader()):\n",
    "    print(\"Masked Molecule\", datamodule.tokenizer.decode(sample['input_ids'].flatten()))\n",
    "    mask = sample['labels'].flatten()!=-100\n",
    "    labels = sample['labels'].flatten()[mask]\n",
    "    print(\"Labels\", datamodule.tokenizer.convert_ids_to_tokens(labels))\n",
    "    pred = trainer.model.model(\n",
    "        input_ids=sample['input_ids'],\n",
    "        attention_mask = sample['attention_mask'],\n",
    "    )\n",
    "    pred = pred.logits[0].argmax(axis=1)[mask]\n",
    "    pred = datamodule.tokenizer.convert_ids_to_tokens(pred)\n",
    "    print(\"Labels\", pred)\n",
    "    print(\"_\"*150)\n",
    "    if step > 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mist_demo",
   "language": "python",
   "name": "mist_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
