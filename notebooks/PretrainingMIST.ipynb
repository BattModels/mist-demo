{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RobertaDataSet' from 'mist.data_modules.roberta_dataset' (/home/abhutani/mist/mist/data_modules/roberta_dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from mist.models.roberta_base import RoBERTa\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmist\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_modules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroberta_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaDataSet\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmist\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_schedule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RelativeCosineWarmup\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# enable RUST based parallelism for tokenizers\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RobertaDataSet' from 'mist.data_modules.roberta_dataset' (/home/abhutani/mist/mist/data_modules/roberta_dataset.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from mist.models.roberta_base import RoBERTa\n",
    "from mist.data_modules.roberta_dataset import RobertaDataSet\n",
    "from mist.utils.lr_schedule import RelativeCosineWarmup\n",
    "\n",
    "# enable RUST based parallelism for tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mist.models import roberta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': 'mist.models.roberta_base',\n",
       " '__doc__': None,\n",
       " '__package__': 'mist.models',\n",
       " '__loader__': <_frozen_importlib_external.SourceFileLoader at 0x14bb245aff10>,\n",
       " '__spec__': ModuleSpec(name='mist.models.roberta_base', loader=<_frozen_importlib_external.SourceFileLoader object at 0x14bb245aff10>, origin='/home/abhutani/mist/mist/models/roberta_base.py'),\n",
       " '__file__': '/home/abhutani/mist/mist/models/roberta_base.py',\n",
       " '__cached__': '/home/abhutani/mist/mist/models/__pycache__/roberta_base.cpython-311.pyc',\n",
       " '__builtins__': {'__name__': 'builtins',\n",
       "  '__doc__': \"Built-in functions, types, exceptions, and other objects.\\n\\nThis module provides direct access to all 'built-in'\\nidentifiers of Python; for example, builtins.len is\\nthe full name for the built-in function len().\\n\\nThis module is not normally accessed explicitly by most\\napplications, but can be useful in modules that provide\\nobjects with the same name as a built-in value, but in\\nwhich the built-in of that name is also needed.\",\n",
       "  '__package__': '',\n",
       "  '__loader__': _frozen_importlib.BuiltinImporter,\n",
       "  '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'),\n",
       "  '__build_class__': <function __build_class__>,\n",
       "  '__import__': <function __import__(name, globals=None, locals=None, fromlist=(), level=0)>,\n",
       "  'abs': <function abs(x, /)>,\n",
       "  'all': <function all(iterable, /)>,\n",
       "  'any': <function any(iterable, /)>,\n",
       "  'ascii': <function ascii(obj, /)>,\n",
       "  'bin': <function bin(number, /)>,\n",
       "  'breakpoint': <function breakpoint>,\n",
       "  'callable': <function callable(obj, /)>,\n",
       "  'chr': <function chr(i, /)>,\n",
       "  'compile': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  'delattr': <function delattr(obj, name, /)>,\n",
       "  'dir': <function dir>,\n",
       "  'divmod': <function divmod(x, y, /)>,\n",
       "  'eval': <function eval(source, globals=None, locals=None, /)>,\n",
       "  'exec': <function exec(source, globals=None, locals=None, /, *, closure=None)>,\n",
       "  'format': <function format(value, format_spec='', /)>,\n",
       "  'getattr': <function getattr>,\n",
       "  'globals': <function globals()>,\n",
       "  'hasattr': <function hasattr(obj, name, /)>,\n",
       "  'hash': <function hash(obj, /)>,\n",
       "  'hex': <function hex(number, /)>,\n",
       "  'id': <function id(obj, /)>,\n",
       "  'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x14bb25dd1250>>,\n",
       "  'isinstance': <function isinstance(obj, class_or_tuple, /)>,\n",
       "  'issubclass': <function issubclass(cls, class_or_tuple, /)>,\n",
       "  'iter': <function iter>,\n",
       "  'aiter': <function aiter(async_iterable, /)>,\n",
       "  'len': <function len(obj, /)>,\n",
       "  'locals': <function locals()>,\n",
       "  'max': <function max>,\n",
       "  'min': <function min>,\n",
       "  'next': <function next>,\n",
       "  'anext': <function anext>,\n",
       "  'oct': <function oct(number, /)>,\n",
       "  'ord': <function ord(c, /)>,\n",
       "  'pow': <function pow(base, exp, mod=None)>,\n",
       "  'print': <function print(*args, sep=' ', end='\\n', file=None, flush=False)>,\n",
       "  'repr': <function repr(obj, /)>,\n",
       "  'round': <function round(number, ndigits=None)>,\n",
       "  'setattr': <function setattr(obj, name, value, /)>,\n",
       "  'sorted': <function sorted(iterable, /, *, key=None, reverse=False)>,\n",
       "  'sum': <function sum(iterable, /, start=0)>,\n",
       "  'vars': <function vars>,\n",
       "  'None': None,\n",
       "  'Ellipsis': Ellipsis,\n",
       "  'NotImplemented': NotImplemented,\n",
       "  'False': False,\n",
       "  'True': True,\n",
       "  'bool': bool,\n",
       "  'memoryview': memoryview,\n",
       "  'bytearray': bytearray,\n",
       "  'bytes': bytes,\n",
       "  'classmethod': classmethod,\n",
       "  'complex': complex,\n",
       "  'dict': dict,\n",
       "  'enumerate': enumerate,\n",
       "  'filter': filter,\n",
       "  'float': float,\n",
       "  'frozenset': frozenset,\n",
       "  'property': property,\n",
       "  'int': int,\n",
       "  'list': list,\n",
       "  'map': map,\n",
       "  'object': object,\n",
       "  'range': range,\n",
       "  'reversed': reversed,\n",
       "  'set': set,\n",
       "  'slice': slice,\n",
       "  'staticmethod': staticmethod,\n",
       "  'str': str,\n",
       "  'super': super,\n",
       "  'tuple': tuple,\n",
       "  'type': type,\n",
       "  'zip': zip,\n",
       "  '__debug__': True,\n",
       "  'BaseException': BaseException,\n",
       "  'BaseExceptionGroup': BaseExceptionGroup,\n",
       "  'Exception': Exception,\n",
       "  'GeneratorExit': GeneratorExit,\n",
       "  'KeyboardInterrupt': KeyboardInterrupt,\n",
       "  'SystemExit': SystemExit,\n",
       "  'ArithmeticError': ArithmeticError,\n",
       "  'AssertionError': AssertionError,\n",
       "  'AttributeError': AttributeError,\n",
       "  'BufferError': BufferError,\n",
       "  'EOFError': EOFError,\n",
       "  'ImportError': ImportError,\n",
       "  'LookupError': LookupError,\n",
       "  'MemoryError': MemoryError,\n",
       "  'NameError': NameError,\n",
       "  'OSError': OSError,\n",
       "  'ReferenceError': ReferenceError,\n",
       "  'RuntimeError': RuntimeError,\n",
       "  'StopAsyncIteration': StopAsyncIteration,\n",
       "  'StopIteration': StopIteration,\n",
       "  'SyntaxError': SyntaxError,\n",
       "  'SystemError': SystemError,\n",
       "  'TypeError': TypeError,\n",
       "  'ValueError': ValueError,\n",
       "  'Warning': Warning,\n",
       "  'FloatingPointError': FloatingPointError,\n",
       "  'OverflowError': OverflowError,\n",
       "  'ZeroDivisionError': ZeroDivisionError,\n",
       "  'BytesWarning': BytesWarning,\n",
       "  'DeprecationWarning': DeprecationWarning,\n",
       "  'EncodingWarning': EncodingWarning,\n",
       "  'FutureWarning': FutureWarning,\n",
       "  'ImportWarning': ImportWarning,\n",
       "  'PendingDeprecationWarning': PendingDeprecationWarning,\n",
       "  'ResourceWarning': ResourceWarning,\n",
       "  'RuntimeWarning': RuntimeWarning,\n",
       "  'SyntaxWarning': SyntaxWarning,\n",
       "  'UnicodeWarning': UnicodeWarning,\n",
       "  'UserWarning': UserWarning,\n",
       "  'BlockingIOError': BlockingIOError,\n",
       "  'ChildProcessError': ChildProcessError,\n",
       "  'ConnectionError': ConnectionError,\n",
       "  'FileExistsError': FileExistsError,\n",
       "  'FileNotFoundError': FileNotFoundError,\n",
       "  'InterruptedError': InterruptedError,\n",
       "  'IsADirectoryError': IsADirectoryError,\n",
       "  'NotADirectoryError': NotADirectoryError,\n",
       "  'PermissionError': PermissionError,\n",
       "  'ProcessLookupError': ProcessLookupError,\n",
       "  'TimeoutError': TimeoutError,\n",
       "  'IndentationError': IndentationError,\n",
       "  'IndexError': IndexError,\n",
       "  'KeyError': KeyError,\n",
       "  'ModuleNotFoundError': ModuleNotFoundError,\n",
       "  'NotImplementedError': NotImplementedError,\n",
       "  'RecursionError': RecursionError,\n",
       "  'UnboundLocalError': UnboundLocalError,\n",
       "  'UnicodeError': UnicodeError,\n",
       "  'BrokenPipeError': BrokenPipeError,\n",
       "  'ConnectionAbortedError': ConnectionAbortedError,\n",
       "  'ConnectionRefusedError': ConnectionRefusedError,\n",
       "  'ConnectionResetError': ConnectionResetError,\n",
       "  'TabError': TabError,\n",
       "  'UnicodeDecodeError': UnicodeDecodeError,\n",
       "  'UnicodeEncodeError': UnicodeEncodeError,\n",
       "  'UnicodeTranslateError': UnicodeTranslateError,\n",
       "  'ExceptionGroup': ExceptionGroup,\n",
       "  'EnvironmentError': OSError,\n",
       "  'IOError': OSError,\n",
       "  'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       "  'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 2000 BeOpen.com.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
       "  All Rights Reserved.,\n",
       "  'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
       "      for supporting Python development.  See www.python.org for more information.,\n",
       "  'license': Type license() to see the full license text,\n",
       "  'help': Type help() for interactive help, or help(object) for help about object.,\n",
       "  'execfile': <function _pydev_bundle._pydev_execfile.execfile(file, glob=None, loc=None)>,\n",
       "  'runfile': <function _pydev_bundle.pydev_umd.runfile(filename, args=None, wdir=None, namespace=None)>,\n",
       "  '__IPYTHON__': True,\n",
       "  'display': <function IPython.core.display_functions.display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, raw=False, clear=False, **kwargs)>,\n",
       "  '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1011__': <capsule object NULL at 0x14bafffa3990>,\n",
       "  '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__': <capsule object NULL at 0x14b9c81bfab0>,\n",
       "  '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1013__': <capsule object NULL at 0x14b9c545ca80>,\n",
       "  'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x14bb2453f910>>}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training Dataset\n",
    "\n",
    "We use a subset of randomly sampled molecules from [Enamine’s REAL Space Chemical Library](https://enamine.net/compound-collections/real-compounds/real-space-navigator), which is currently the largest library of commercially available compounds with 48B virtual products based on ~0.1M reagents and building blocks and 166 defined chemical rules to combine them. \n",
    "\n",
    "This pre-training dataset covers a significant fraction of the space of possible molecules. The plot below visualizes the chemical space covered by the pre-training dataset using the [TMAP](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-0416-x) (Tree Manifold Approximation and Projection) algorithm and compare it to the chemical space convered by datasets in MoleculeNet. MoleculeNet is a popular cheminformatics benchmark and is representative of datasets typically used to train machine learning models for chemistry.\n",
    "\n",
    "<img src=\"figures/MIST_TMAP.png\" alt=\"tmap\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">\n",
    "\n",
    "\n",
    "The molecules are stored as SMILES (Simplified Molecular-Input Line-Entry System) strings. SMILES are a cheminformatic line notation for describing chemical structures using short ASCII strings. SMILES strings are like a connection table in that they identify the nodes and edges of a molecular graph. In SMILES, hydrogen are typically implicitly implied and atoms are represented by their atomic symbol enclosed in brackets unless they are elements of the “organic subset” (`B`, `C`, `N`, `O`, `P`, `S`, `F`, `Cl`,`Br`, and `I`), which do not require brackets unless they are charged. So gold would be `[Au]` but chlorine would be just `Cl`. If hydrogens are explicitly implied brackets are used. A formal charge is represented by one of the symbols `+` or `-`. Single, double, triple, and aromatic bonds are represented by the symbols, `-`, `=`, `#`, and `:`, respectively. Single and aromatic bonds may be, and usually are, omitted. Below is an example of a SMILES string and the corresponding 2D molecular graph.\n",
    "\n",
    "<img src=\"figures/smiles.png\" alt=\"smiles\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./sample_data/\"\n",
    "tokenizer = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "mlm_probability = 0.15 \n",
    "batch_size = 64\n",
    "val_batch_size = 1\n",
    "\n",
    "datamodule = RobertaDataSet(\n",
    "    path=pretraining_data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    val_batch_size=val_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = datamodule.tokenizer.vocab_size\n",
    "max_position_embeddings = 512\n",
    "num_attention_heads = 12\n",
    "num_attention_heads = 6\n",
    "num_hidden_layers = 6\n",
    "hidden_size = 768\n",
    "intermediate_size = 768\n",
    "relative_cosine_scheduler = lambda optimizer: RelativeCosineWarmup(optimizer, num_warmup_steps=\"beta2\", num_training_steps=50_000)\n",
    "\n",
    "model = RoBERTa(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_position_embeddings,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    optimizer = torch.optim.AdamW,\n",
    "    lr_schedule = relative_cosine_scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some callbacks are defined for convinience\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\") # monitors and logs learning rate for schedulers during training\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_last=\"link\",\n",
    "        filename=\"epoch={epoch}-step={step}-val_loss={val/loss_epoch:.2f}\",\n",
    "        monitor=\"val/loss_epoch\",\n",
    "        save_top_k=5,\n",
    "        verbose=True,\n",
    "        auto_insert_metric_name=False\n",
    "    ) # saves the best model during training based on validation loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "The pre-training strategy we use is analogous the MLM (Masked Language Modeling) used in NLP (Natural Language Processing). \n",
    "Part of the SMILES string is replace with a 'mask'. The objective is a cross-entropy loss on predicting the masked tokens.\n",
    "\n",
    "<img src=\"figures/MIST_pretraining.png\" alt=\"tmap\" width=\"50%\" display=\"block\" margin-left=\"auto;\" margin-right=\"auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/abhutani/electrolyte_fm/.venv/lib/python3.11/s ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:54671 (errno: 97 - Address family not supported by protocol).\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | RobertaForMaskedLM | 24.1 M\n",
      "---------------------------------------------\n",
      "24.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.1 M    Total params\n",
      "96.335    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                           | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/abhutani/electrolyte_fm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████| 500/500 [01:15<00:00,  6.63it/s, v_num=12, train/loss_step=1.040]\n",
      "Validation: |                                                              | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|                                                         | 0/10 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|                                            | 0/10 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  10%|███▌                                | 1/10 [00:00<00:00, 15.53it/s]\n",
      "Validation DataLoader 0:  20%|███████▏                            | 2/10 [00:00<00:00, 14.28it/s]\n",
      "Validation DataLoader 0:  30%|██████████▊                         | 3/10 [00:00<00:00, 15.58it/s]\n",
      "Validation DataLoader 0:  40%|██████████████▍                     | 4/10 [00:00<00:00, 17.06it/s]\n",
      "Validation DataLoader 0:  50%|██████████████████                  | 5/10 [00:00<00:00, 17.70it/s]\n",
      "Validation DataLoader 0:  60%|█████████████████████▌              | 6/10 [00:00<00:00, 17.99it/s]\n",
      "Validation DataLoader 0:  70%|█████████████████████████▏          | 7/10 [00:00<00:00, 18.35it/s]\n",
      "Validation DataLoader 0:  80%|████████████████████████████▊       | 8/10 [00:00<00:00, 18.62it/s]\n",
      "Validation DataLoader 0:  90%|████████████████████████████████▍   | 9/10 [00:00<00:00, 17.77it/s]\n",
      "Validation DataLoader 0: 100%|███████████████████████████████████| 10/10 [00:00<00:00, 18.30it/s]\n",
      "Epoch 0: 100%|█| 500/500 [01:16<00:00,  6.53it/s, v_num=12, train/loss_step=1.040, val/loss_step="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 500: 'val/loss_epoch' reached inf (best inf), saving model to '/home/abhutani/electrolyte_fm/lightning_logs/version_12/checkpoints/epoch=0-step=500-val_loss=nan.ckpt' as top 5\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 500/500 [01:19<00:00,  6.26it/s, v_num=12, train/loss_step=1.040, val/loss_step=\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    precision = \"16-mixed\", # Combines FP32 and lower-bit floating points to reduce memory footprint and increase performance.\n",
    "    strategy = \"ddp_notebook\", # Distributed Data Parallel training.\n",
    "    use_distributed_sampler = False,  # Handled by DataModule (needed due to IterableDataset).\n",
    "    limit_train_batches=500, \n",
    "    limit_val_batches=10, \n",
    "    max_epochs=1, \n",
    "    devices=torch.cuda.device_count(),\n",
    "    callbacks=[lr_monitor, checkpoint_callback])\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Molecule <bos>CCS(=O<mask>(=[77Kr])NCC1=CN(CC<mask>CCOCCNC(=<mask>)CN2N=C<mask>C[AlH4-]C<mask>N3C(=O)C2=O)N=N1<eos>\n",
      "Labels ['=', ')', 'O', 'O', 'O', '3', 'O', 'C']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>COC(=O)C(<mask>C1=CN(CCN2CCC(N(C)C)C<mask><mask>N=N1)<mask>C(=<mask>)<mask>N1C=C(C<mask>N)=O)C[117Sn+4]<mask>O)<mask>C1=O<eos>\n",
      "Labels ['C', 'N', '2', ')', 'N', 'O', 'C', '(', '(', '=', 'N']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>NC<mask>=O)CCC(N<mask>(=O)(=O)C1=CC=C(Cl<mask>C<mask>C1<mask>C<mask><mask><mask>)N[C@@H]<mask>CO)CNC(=<mask>)CN1N=CC(=O<mask>NC1=O<eos>\n",
      "Labels ['(', 'C', 'S', ')', '=', ')', '(', '=', 'O', '(', 'O', ')']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>CN<mask>C(=O<mask>C=CN(CC(=O<mask>NCC(O)C<mask>C(=O)C(CC(<mask>)=O)NC(<mask>O)CC2=CC=CC(F)=C2)C1=O<eos>\n",
      "Labels ['1', ')', ')', 'N', 'N', '=', ')', 'C']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Masked Molecule <bos>CN<mask>C(C(<mask>C<mask>CN([137Ba](C)(=O)=O)C[Co-])C2=NN=NN2CC<mask>2CCO<mask>C2<mask>=NC2=C1C(=O)<mask>C(=<mask>)N2C<eos>\n",
      "Labels ['1', 'N', '2', 'S', '2', 'N', 'C', ')', 'N', 'O']\n",
      "Labels ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for step, sample in enumerate(datamodule.val_dataloader()):\n",
    "    print(\"Masked Molecule\", datamodule.tokenizer.decode(sample['input_ids'].flatten()))\n",
    "    mask = sample['labels'].flatten()!=-100\n",
    "    labels = sample['labels'].flatten()[mask]\n",
    "    print(\"Labels\", datamodule.tokenizer.convert_ids_to_tokens(labels))\n",
    "    pred = model(\n",
    "        sample\n",
    "    )\n",
    "    pred = pred.logits[0].argmax(axis=1)[mask]\n",
    "    pred = datamodule.tokenizer.convert_ids_to_tokens(pred)\n",
    "    print(\"Labels\", pred)\n",
    "    print(\"_\"*200)\n",
    "    if step > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mist_demo",
   "language": "python",
   "name": "mist_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
